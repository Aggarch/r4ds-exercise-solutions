---
title: "Untitled"
author: "Jeffrey Arnold"
date: "1/10/2018"
output: html_document
---

# Visualize

Other resources

- [All the graph things](http://stat545.com/graph00_index.html) Stat 545. Jenny Bryan.



# Tibbles

Why might you want to create non-syntactic variable names? 
Since variable names are often used as in plots (e.g. axis-titles) or headers in tables, where having spaces or other characters that are invalid R variable names is useful.
Those functions will have ways to use text other than the column.



Why might you want to create non-syntactic variable names? 
Since variable names are often used as in plots (e.g. axis-titles) or headers in tables, where having spaces or other characters that are invalid R variable names is useful.
Those functions will have ways to use text other than the column.

Discuss the definition of a data frame.

What is the traditional R `data.frame`?

In general, discuss how this "dialect" of R relates to base R and other R that they will see.

Also, need to discuss types of variables.

If `nycflights::flights` were printed in the console it would be much worse. Just try it, I dare you.
```{r eval=FALSE}
as.data.frame(nycflights13::flights) 
```

### Subsetting

**Note** Warnings about partial matching! What is it and why is it dangerous.

### Interacting with Older Code


**Note** Not all older functions work with tibbles (an example includes giAmelia); usually because they rely on quirks in `data.frame` behavior that `tibbles` "fix". Use `as.data.frame()` to turn a tibble back into a `data.frame`. 
This is usually because of `[` and the way it inconsistenly returns a vector or a data frame.
With tibbles `[` always returns a data frame

# Tidy


**NOTES**

- Add [Tidy Data](http://www.jstatsoft.org/v59/i10/paper) to reading
- Use COW war dataset as an example of non-tidy data
- Also WDI data for non-tidy data
- Replication datatsets are often non-tidy. Why?
- See this post by [Jeff Leek](http://simplystatistics.org/2016/02/17/non-tidy-data/)

The Rules

1. Each **variable** has its own **column**
2. Each **observation** muust have its own **row**
3. Each **value** must have its own **cell**

or even

1. Put each **dataset** in a **tibble**
2. Put each **variable** in a **column**

These seem obvious at first, so we need to see examples of not-following tidy data and what happens.

Some nuances:

The definitions of **variable**, **observation**, and **value** are not always clear. And how you store and arrange the data can depend on how you aim to use it. Generally, aim for storing the data in a tidy format that ensures minimal errors. When you model it, you can transform the data later.
See non-tidy data.

It is easier to work with variables in columns because of `mutate` and `summary` functions.
It will also work better with `tidyverse` functions: e.g. using `group_by` to group and summarize, or `facet_*` and aesthetics in **ggplot2**.

The tidy data ideas are adapted from the [database normalization](https://en.wikipedia.org/wiki/Database_normalization), but simplified and adapted to the general uses of practicing data scientists.

## Relational Data

# Dates and Times

- Ideas for applications: CDB90 data, COW war start end and duration 
- Read more on time-zones: https://en.wikipedia.org/wiki/Time_zone
- Computerphile [The Problem with Time & Timezones - Computerphile](https://www.youtube.com/watch?v=-5wpm-gesOY)
- The history of the tz database are themselves interesting: https://en.wikipedia.org/wiki/Tz_database
- [A literary appreciation of the Olson/Zoneinfo/tz database]( https://blog.jonudell.net/2009/10/23/a-literary-appreciation-of-the-olsonzoneinfotz-database/)

I think time-zones are likely a point for social science research in and of themselves. Policy choices. Coordination. Regression discontinuity designs. Just sayin...

## Relational Data

**NOTES**

[nycflights13](https://cran.r-project.org/web/packages/nycflights13/index.html) is an example of a **data-only** R package. R packages can contain both functions and data.
Since data-sets can get large, often they can be packaged as their own dataset. These sorts of data-only R packages make it convenient for R users to access your data, but it should not be the only way you provide your research data. Not everyone uses R, so the original data should be provided in a program agnostic format (e.g. csv files). This also holds for those using Stata; they should not be distributing data in `.dta` format files specific to Stata (even if as we saw earlier, other programs can read that data.)
Another example of a data-only R package is [gapminder](https://cran.r-project.org/package=gapminder).

How does Hadley create his diagrams? 

The four tables in the **nycflights13** package:
```{r}
airlines
airports
planes
weather
```

## Strings

Functions and packages coverered

- **stringr** package
- `str_length`
- `str_c`
- `str_replace_na`
- `str_sub`
- `str_to_uppser`, `str_sort`, `str_to_lower`, `str_order`
- `str_length`, `str_pad`, `str_trim`, `str_sub`
- For regex = `str_view`, `str_view_all`
- regex syntax
- `str_detect`
- `str_subset`
- `str_count`
- `str_extract`
- `str_match`
- `tidyr::extract`
- `str_split`
- `str_locate`
- `str_sub`
- the **stringi** package

Ideas

- mention [`rex`](https://github.com/kevinushey/rex). A package with friendly regular expressions.
- Use it to match country names? Extract numbers from text?
- Discuss fuzzy joining and string distance, approximate matching.

Also see 

- [Character encoding](http://stat545.com/block032_character-encoding.html) Stat 545. Jenny Bryan.
- [Character data](http://stat545.com/block028_character-data.html). Stat 545. Jenny Bryan.
- [Regular expression in R](http://stat545.com/block022_regular-expression.html). Stat 545. Jenny Bryan.

## Dates and Times

**NOTE** This section seems less complete than the others. 
Refer to the [lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) vignette for more information.

## Vectors

Why does this matter? 99% of the time in the work you do, it won't.
Someone else has written the numerical methods and (hopefully) accounted for these issues.
And the types of problems you encounter in social science generally are not dealing with these issues.
However, if you aren't even aware that "floating point numbers" are a "thing", if something goes wrong, it will seem like magic.
Also, at least being aware of these problems will help you understand error messages from optimization routines that complaing of "numerical precision".

Computerphile has a good video on [Floating Point Numbers](https://youtu.be/PZRI1IfStY0).


# Models

Some of the discussion of models is slightly different, and has a different emphasis than in most social science research.
This is largely because this book is speaking to data scientists, where the primary goal is prediction rather than theory testing (that I don't view these as too different is a different story).

The discussion about hypothesis generation vs. confirmation is interesting.
Too little emphasis is placed on hypothesis generation in social science.
The importance of out of sample testing also receives too little emphasis in political science.

And from this discussion it should be clear that many papers in social science are hypothesis generation masquerading as hypothesis confirmation.

# Model Basics

Distinction between *family of models* and *fitted model* is a useful way to think about models. 
Especially as we can abstract some families of models to be themselves a fitted model of a more flexible family of models.
For example, linear regression is a special case of GLM or Gaussian Processes etc.

**NOTE** It's worth mentioning these as more general models. Though they don't appear as much in social science work. I should try to explain that. I can think of several reasons

- preference for easy to explain models (though I think that's wrong--most people can't visualize high-dimensional space well, and interpret results marginally even though they are conditional)
- status-quo bias and path dependence combined with lack of knowledge of work outside the field and median lack of technical ability to understand or use these models.
- the most principled reason is that those modre complicated models really excel in prediction. If we take an agnostic approach to regression, as in the Angrist and Pischke books, then regression isn't being used to fit $f(y | x)$, its being used to fit $E(f(y | x))$, and more specifically to get some sort of average effect for a change in a specific variable.

# Strings

*Notes* This is detailed, but these details can make your life hell. Skim now, but be aware that what should be simple, actually is not.

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">In data analysis, ��% is data cleaning, ��% is modeling, and the rest is character encoding issues</p>&mdash; Jeffrey B. Arnold (@jrnld) <a href="https://twitter.com/jrnld/status/759608460677832706">July 31, 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

This Computerphile video on Unicode is great [Characters, Symbols and the Unicode Miracle - Computerphile](https://www.youtube.com/watch?v=MijmeoH9LT4)

Note that these issues are real. Reusing one of Chris Adolph's csv files from an earlier version of this course gave me problems, resulting in me filing this [bug report](https://github.com/tidyverse/readr/issues/111).

The suggested reading is very useful: http://kunststube.net/encoding/

This becomes especially useful when you take "Text as Data".

```{r}
charToRaw("Jeff")
class(charToRaw("Jeff"))
```

# MOdel basics

More complicated models can be visualized with

1. predictions
2. residuals

Notes

- look at `tidyr::complete`, `tidyr::expand`, and `modelr::data_grid` functions
- `modelr::add_residuals` and `modelr::add_predictions` functions add residuals or predictions to the original data
- `geom_ref_line`
