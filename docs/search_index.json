[
["index.html", "Exercise Solutions and Notes for “R for Data Science” Welcome", " Exercise Solutions and Notes for “R for Data Science” Jeffrey B. Arnold Welcome This contains my exercise solutions and notes for Hadley Wickham and Garret Grolemund, R for Data Science. The original website is at r4ds.had.co.nz. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["visualize.html", "1 Visualize 1.1 Introduction 1.2 Position Adjustments 1.3 Coordinate Systems", " 1 Visualize 1.1 Introduction 1.1.1 Prerequisites library(&quot;tidyverse&quot;) 1.1.2 First Steps 1.1.2.1 Exercises Run ggplot(data = mpg) what do you see? ggplot(data = mpg) Nothing. The plot is created, but ggplot is not given any data to plot. How many rows are in mtcars? How many columns? nrow(mtcars) #&gt; [1] 32 ncol(mtcars) #&gt; [1] 11 This can also be found by printing the dataset, or looking in the environment pane. What does the drv variable describe? Read the help for ?mpg to find out. ?mpg The drv variable takes the following values f = front-wheel drive r = rear wheel drive 4 = 4wd Make a scatterplot of hwy vs cyl ggplot(mpg, aes(x = hwy, y = cyl)) + geom_point() What happens if you make a scatterplot of class vs drv. Why is the plot not useful? ggplot(mpg, aes(x = class, y = drv)) + geom_point() A scatterplot is not a useful way to plot these variables, since both drv and class are factor variables, and the scatterplot cannot show which are overlapping or not. count(mpg, drv, class) #&gt; Source: local data frame [12 x 3] #&gt; Groups: drv [?] #&gt; #&gt; drv class n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 4 compact 12 #&gt; 2 4 midsize 3 #&gt; 3 4 pickup 33 #&gt; 4 4 subcompact 4 #&gt; 5 4 suv 51 #&gt; 6 f compact 35 #&gt; # ... with 6 more rows 1.1.3 Aesthetic mappings 1.1.3.1 Exercises What’s gone wrong with this code? Why are the points not blue? ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;)) Since color = &quot;blue&quot; was included within the mapping argument, it was treated as an aesthetic (a mapping between a variable and a value). It was treated as a variable which has only one value: “blue”. Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg? ?mpg When printing the data frame, this information is given at the top of each column within angled brackets. Categorical variables have a class of “character” (&lt;chr&gt;). mpg #&gt; # A tibble: 234 × 11 #&gt; manufacturer model displ year cyl trans drv cty hwy fl #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p #&gt; 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p #&gt; 3 audi a4 2.0 2008 4 manual(m6) f 20 31 p #&gt; 4 audi a4 2.0 2008 4 auto(av) f 21 30 p #&gt; 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p #&gt; 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p #&gt; # ... with 228 more rows, and 1 more variables: class &lt;chr&gt; The glimpse command from “mpg” shows this: glimpse(mpg) #&gt; Observations: 234 #&gt; Variables: 11 #&gt; $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;... #&gt; $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 qua... #&gt; $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0,... #&gt; $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1... #&gt; $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6... #&gt; $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)... #&gt; $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;,... #&gt; $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 1... #&gt; $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 2... #&gt; $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;,... #&gt; $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;comp... Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? The variable cty, city highway miles per gallon, is a continuous variable: ggplot(mpg, aes(x = displ, y = hwy, color = cty)) + geom_point() Instead of using discrete colors, the continous variable uses a scale that goes from black to bluish. ggplot(mpg, aes(x = displ, y = hwy, size = cty)) + geom_point() When mapped to size, the sizes of the points vary continuously with respect to the size (although the legend shows a few representative values) ggplot(mpg, aes(x = displ, y = hwy, shape = cty)) + geom_point() #&gt; Error: A continuous variable can not be mapped to shape When a continuous value is mapped to shape, it gives an error. Though we could split a continuous variable into discrete categories and use shape, this would conceptually not make sense. A continuous numeric variable is ordered, but shapes have no natural order. It is clear that smaller points correspond to smaller values, or once the color scale is given, which points are larger or smaller. But it is not clear whether a square is greater or less than a circle. What happens if you map the same variable to multiple aesthetics? ggplot(mpg, aes(x = displ, y = hwy, color = hwy, size = displ)) + geom_point() In the above plot, hwy is mapped to both location on the y-axis and color, and displ is mapped to both location on the x-axis and size. The code works and produces a plot, even if it is a bad one. Mapping a single variable to multiple aesthetics is redundant. Because it is redundant information, in most cases avoid mapping a single variable to multiple aesthetics. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point) The following example is given in ?geom_point: ggplot(mtcars, aes(wt, mpg)) + geom_point(shape = 21, colour = &quot;black&quot;, fill = &quot;white&quot;, size = 5, stroke = 5) Stroke changes the color of the border for shapes (22-24). What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ &lt; 5)? ggplot(mpg, aes(x = displ, y = hwy, colour = displ &lt; 5)) + geom_point() Aesthetics can also be mapped to expressions (code like displ &lt; 5). It will create a temporary variable which takes values from the result of the expression. In this case, it is logical variable which is TRUE or FALSE. This also explains exercise 1, color = &quot;blue&quot; created a categorical variable that only had one category: “blue”. 1.1.4 Facets 1.1.4.1 Exercises What happens if you facet on a continuous variable? Let’s see ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + facet_grid(. ~ cty) It converts the continuous varible to a factor and creates facets for all unique values of it. What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot? They are cells in which there are no values of the combination of drv and cyl. ggplot(data = mpg) + geom_point(mapping = aes(x = drv, y = cyl)) The locations in the above plot without points are the same cells in facet_grid(drv ~ cyl) tha have no points. What plots does the following code make? What does . do? The symbol . ignores that dimension for faceting. This plot facets by values of drv on the y-axis: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ .) This plot facets by values of cyl on the x-axis: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(. ~ cyl) Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol variables? The arguments nrow (ncol) determines the number of rows (columns) to use when laying out the facets. It is necessary since facet_wrap only facets on one variable. These arguments are unnecessary for facet_grid since the number of rows and columns are determined by the number of unique values of the variables specified. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why? You should put the variable with more unique levels in the columns if the plot is laid out landscape. It is easier to compare relative levels of y by scanning horizontally, so it may be easier to visually compare these levels. I’m actually not sure about the correct answer to this. 1.1.5 Geometric Objects What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter? NOTE This doesn’t appear earlier in the chapter Issue #510 What does the se argument to geom_smooth() do? It adds standard error bands to the lines. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(se = TRUE) #&gt; `geom_smooth()` using method = &#39;loess&#39; By default se = TRUE: ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; Will these two graphs look different? Why/why not? No. Because both geom_point and geom_smooth use the same data and mappings. They will inherit those options from the ggplot object, and thus don’t need to specified again (or twice). ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) #&gt; `geom_smooth()` using method = &#39;loess&#39; Recreate the R code necessary to generate the following graphs. ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + geom_smooth(mapping = aes(group = drv), se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + geom_point() + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(mapping = aes(colour = drv)) + geom_smooth(se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot(mpg, aes(x = displ, y = hwy)) + geom_point(aes(colour = drv)) + geom_smooth(aes(linetype = drv), se = FALSE) #&gt; `geom_smooth()` using method = &#39;loess&#39; ggplot(mpg, aes(x = displ, y = hwy, fill = drv)) + geom_point(color = &quot;white&quot;, shape = 21) 1.1.6 Statistical Transformations What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function? The default geom for stat_summary is geom_pointrange (see the stat) argument. But, the default stat for geom_pointrange is identity, so use geom_pointrange(stat = &quot;summary&quot;). ggplot(data = diamonds) + geom_pointrange( mapping = aes(x = cut, y = depth), stat = &quot;summary&quot;, ) #&gt; No summary function supplied, defaulting to `mean_se() The default message says that stat_summary uses the mean and sd to calculate the point, and range of the line. So lets use the previous values of fun.ymin, fun.ymax, and fun.y: ggplot(data = diamonds) + geom_pointrange( mapping = aes(x = cut, y = depth), stat = &quot;summary&quot;, fun.ymin = min, fun.ymax = max, fun.y = median ) What does geom_col() do? How is it different to geom_bar()? geom_col differs from geom_bar in its default stat. geom_col has uses the identity stat. So it expects that a variable already exists for the height of the bars. geom_bar uses the count stat, and so will count observations in groups in order to generate the variable to use for the height of the bars. Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common? See the ggplot2 documentation What variables does stat_smooth() compute? What parameters control its behaviour? stat_smooth calculates y: predicted value ymin: lower value of the confidence interval ymax: upper value of the confidence interval se: standard error There’s parameters such as method which determines which method is used to calculate the predictions and confidence interval, and some other arguments that are passed to that. In our proportion bar chart, we need to set group = 1 Why? In other words what is the problem with these two graphs? If group is not set to 1, then all the bars have prop == 1. The function geom_bar assumes that the groups are equal to the x values, since the stat computes the counts within the group. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) The problem with these two plots is that the proportions are calculated within the groups. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..)) This is more likely what was intended: ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop.., group = color)) 1.2 Position Adjustments What is the problem with this plot? How could you improve it? There is overplotting because there are multiple observations for each combination of cty and hwy. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() I’d fix it by using a jitter positition adjustment. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = &quot;jitter&quot;) What parameters to geom_jitter() control the amount of jittering? From the position_jitter documentation, there are two arguments to jitter: width and height, which control the amount of vertical and horizontal jitter. No horizontal jitter ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = position_jitter(width = 0)) Way too much vertical jitter ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = position_jitter(width = 0, height = 15)) Only horizontal jitter: ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = position_jitter(height = 0)) Way too much horizontal jitter: ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point(position = position_jitter(height = 0, width = 20)) Compare and contrast geom_jitter() with geom_count(). What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it. The default position for geom_boxplot is position_dodge (see its docs). When we add color = class to the boxplot, the different classes within drv are placed side by side, i.e. dodged. If it was position_identity, they would be overlapping. ggplot(data = mpg, aes(x = drv, y = hwy, color = class)) + geom_boxplot() ggplot(data = mpg, aes(x = drv, y = hwy, color = class)) + geom_boxplot(position = &quot;identity&quot;) 1.3 Coordinate Systems 1.3.1 Exercises Turn a stacked bar chart into a pie chart using coord_polar(). This is a stacked bar chart with a single category ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar() See the documentation for coord_polar for an example of making a pie chart. In particular, theta = &quot;y&quot;, meaning that the angle of the chart is the y variable has to be specified. ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar(width = 1) + coord_polar(theta = &quot;y&quot;) If theta = &quot;y&quot; is not specified, then you get a bullseye chart ggplot(mpg, aes(x = factor(1), fill = drv)) + geom_bar(width = 1) + coord_polar() If you had a multiple stacked bar chart, like, ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) you end up with a multi-donut chart ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) What does labs() do? Read the documentation. labs is a shortcut function to add labels to different scales. ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() + labs(y = &quot;Highway MPG&quot;, x = &quot;&quot;) What’s the difference between coord_quickmap() and coord_map()? See the docs: coord_map uses a 2D projection: by default the Mercatur project of the sphere to the plot. But this requires transforming all geoms. coord_quickmap uses a quick approximation by using the lat/long ratio as an approximation. This is “quick” because the shapes don’t need to be transformed. What does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do? The coordinates coord_fixed ensures that the abline is at a 45 degree angle, which makes it easy to compare the highway and city mileage against what it would be if they were exactly the same. ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline() + coord_fixed() If we didn’t include geom_point, then the line is no longer at 45 degrees: ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + geom_point() + geom_abline() "],
["workflow-basics.html", "2 Workflow Basics 2.1 Practice", " 2 Workflow Basics 2.1 Practice 2.1.1 Exercises Why does this code not work? my_variable &lt;- 10 my_varıable #&gt; Error in eval(expr, envir, enclos): object &#39;my_varıable&#39; not found The variable being printed is my_varıable, not my_variable: the seventh character is “ı” (LATIN SMALL LETTER DOTLESS I) not “i”. While it wouldn’t have helped much in this case, the importance of distinguishing characters in code is reasons why fonts which clearly distinguish similar characters are preferred in programming: especially important are distinguishing between zero (0), Latin small letter O (o), and Latin capital letter O (O); and the numeral one (1), Latin small letter I (i), Latin capital letter I (i), and Latin small letter L (l). In these fonts, zero and the Latin letter O are often distinguished by using a glyph for zero that uses either a dot in the interior or a slash through it. Also note that the error messages of the form “object ‘…’ not found”, mean just what they say, the object can’t be found by R. This is usually because you either (1) forgot to define the function (or had an error that prevented it from being defined earlier), (2) didn’t load a package with the object, or (3) made a typo in the object’s name (either when using it or when you originally defined it). Tweak each of the following R commands so that they run correctly: library(tidyverse) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats ggplot(dota = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) #&gt; Error in structure(list(data = data, layers = list(), scales = scales_list(), : argument &quot;data&quot; is missing, with no default The error message is argument &quot;data&quot; is missing, with no default. It looks like a typo, dota instead of data. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) fliter(mpg, cyl = 8) #&gt; Error in eval(expr, envir, enclos): could not find function &quot;fliter&quot; R could not find the function fliter because we made a typo: fliter instead of filter. filter(mpg, cyl = 8) #&gt; Error: filter() takes unnamed arguments. Do you need `==`? We aren’t done yet. But the error message gives a suggestion. Let’s follow it. filter(mpg, cyl == 8) #&gt; # A tibble: 70 × 11 #&gt; manufacturer model displ year cyl trans drv cty #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 audi a6 quattro 4.2 2008 8 auto(s6) 4 16 #&gt; 2 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 #&gt; 3 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 11 #&gt; 4 chevrolet c1500 suburban 2wd 5.3 2008 8 auto(l4) r 14 #&gt; 5 chevrolet c1500 suburban 2wd 5.7 1999 8 auto(l4) r 13 #&gt; 6 chevrolet c1500 suburban 2wd 6.0 2008 8 auto(l4) r 12 #&gt; # ... with 64 more rows, and 3 more variables: hwy &lt;int&gt;, fl &lt;chr&gt;, #&gt; # class &lt;chr&gt; filter(diamond, carat &gt; 3) #&gt; Error in filter_(.data, .dots = lazyeval::lazy_dots(...)): object &#39;diamond&#39; not found R says it can’t find the object diamond. This is a typo; the data frame is named diamonds. filter(diamonds, carat &gt; 3) #&gt; # A tibble: 32 × 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.01 Premium I I1 62.7 58 8040 9.10 8.97 5.67 #&gt; 2 3.11 Fair J I1 65.9 57 9823 9.15 9.02 5.98 #&gt; 3 3.01 Premium F I1 62.2 56 9925 9.24 9.13 5.73 #&gt; 4 3.05 Premium E I1 60.9 58 10453 9.26 9.25 5.66 #&gt; 5 3.02 Fair I I1 65.2 56 10577 9.11 9.02 5.91 #&gt; 6 3.01 Fair H I1 56.1 62 10761 9.54 9.38 5.31 #&gt; # ... with 26 more rows How did I know? I started typing in diamond and RStudio autocorrected it to diamonds. Since diamonds includes the variable carat and the code works, that appears to have been the problem. Press Alt + Shift + K. What happens? How can you get to the same place using the menus? This gives a menu with keyboard shortcuts. This can be found in the menu under Tools -&gt; Keyboard Shortcuts Help. "],
["data-transformation.html", "3 Data Transformation 3.1 Prerequisites 3.2 Filter 3.3 Exercises 3.4 Arrange 3.5 Mutate 3.6 Grouped summaries with summarise() 3.7 Grouped mutates and filters", " 3 Data Transformation 3.1 Prerequisites library(nycflights13) library(tidyverse) #&gt; Loading tidyverse: ggplot2 #&gt; Loading tidyverse: tibble #&gt; Loading tidyverse: tidyr #&gt; Loading tidyverse: readr #&gt; Loading tidyverse: purrr #&gt; Loading tidyverse: dplyr #&gt; Conflicts with tidy packages ---------------------------------------------- #&gt; filter(): dplyr, stats #&gt; lag(): dplyr, stats 3.2 Filter glimpse(flights) #&gt; Observations: 336,776 #&gt; Variables: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 7... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 7... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;,... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 94... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, ... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013... 3.3 Exercises Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive) Had an arrival delay of two or more hours Since delay is in minutes, we are looking for flights where arr_delay &gt; 120: flights %&gt;% filter(arr_delay &gt; 120) #&gt; # A tibble: 10,034 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 811 630 101 1047 #&gt; 2 2013 1 1 848 1835 853 1001 #&gt; 3 2013 1 1 957 733 144 1056 #&gt; 4 2013 1 1 1114 900 134 1447 #&gt; 5 2013 1 1 1505 1310 115 1638 #&gt; 6 2013 1 1 1525 1340 105 1831 #&gt; # ... with 1.003e+04 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Flew to Houston (IAH or HOU): flights %&gt;% filter(dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) #&gt; # A tibble: 9,313 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 623 627 -4 933 #&gt; 4 2013 1 1 728 732 -4 1041 #&gt; 5 2013 1 1 739 739 0 1104 #&gt; 6 2013 1 1 908 908 0 1228 #&gt; # ... with 9,307 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Were operated by United, American, or Delta The variable carrier has the airline: but it is in two-digit carrier codes. However, we can look it up in the airlines dataset. airlines #&gt; # A tibble: 16 × 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E Endeavor Air Inc. #&gt; 2 AA American Airlines Inc. #&gt; 3 AS Alaska Airlines Inc. #&gt; 4 B6 JetBlue Airways #&gt; 5 DL Delta Air Lines Inc. #&gt; 6 EV ExpressJet Airlines Inc. #&gt; # ... with 10 more rows Since there are only 16 rows, its not even worth filtering. Delta is DL, American is AA, and United is UA: filter(flights, carrier %in% c(&quot;AA&quot;, &quot;DL&quot;, &quot;UA&quot;)) #&gt; # A tibble: 139,504 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 554 600 -6 812 #&gt; 5 2013 1 1 554 558 -4 740 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # ... with 1.395e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Departed in summer (July, August, and September) The variable month has the month, and it is numeric. filter(flights, between(month, 7, 9)) #&gt; # A tibble: 86,326 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 #&gt; 2 2013 7 1 2 2359 3 344 #&gt; 3 2013 7 1 29 2245 104 151 #&gt; 4 2013 7 1 43 2130 193 322 #&gt; 5 2013 7 1 44 2150 174 300 #&gt; 6 2013 7 1 46 2051 235 304 #&gt; # ... with 8.632e+04 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Arrived more than two hours late, but didn’t leave late filter(flights, !is.na(dep_delay), dep_delay &lt;= 0, arr_delay &gt; 120) #&gt; # A tibble: 29 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 27 1419 1420 -1 1754 #&gt; 2 2013 10 7 1350 1350 0 1736 #&gt; 3 2013 10 7 1357 1359 -2 1858 #&gt; 4 2013 10 16 657 700 -3 1258 #&gt; 5 2013 11 1 658 700 -2 1329 #&gt; 6 2013 3 18 1844 1847 -3 39 #&gt; # ... with 23 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Were delayed by at least an hour, but made up over 30 minutes in flight filter(flights, !is.na(dep_delay), dep_delay &gt;= 60, arr_delay &lt; 30) #&gt; # A tibble: 206 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 3 1850 1745 65 2148 #&gt; 2 2013 1 3 1950 1845 65 2228 #&gt; 3 2013 1 3 2015 1915 60 2135 #&gt; 4 2013 1 6 1019 900 79 1558 #&gt; 5 2013 1 7 1543 1430 73 1758 #&gt; 6 2013 1 11 1020 920 60 1311 #&gt; # ... with 200 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Departed between midnight and 6am (inclusive). filter(flights, dep_time &gt;= 0, dep_time &lt;= 600) #&gt; # A tibble: 9,344 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # ... with 9,338 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; or using between (see next question) filter(flights, between(dep_time, 0, 600)) #&gt; # A tibble: 9,344 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 544 545 -1 1004 #&gt; 5 2013 1 1 554 600 -6 812 #&gt; 6 2013 1 1 554 558 -4 740 #&gt; # ... with 9,338 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? between(x, left, right) is equivalent to x &gt;= left &amp; x &lt;= right. I already used it in 1.4. How many flights have a missing dep_time? What other variables are missing? What might these rows represent? filter(flights, is.na(dep_time)) #&gt; # A tibble: 8,255 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA #&gt; 2 2013 1 1 NA 1935 NA NA #&gt; 3 2013 1 1 NA 1500 NA NA #&gt; 4 2013 1 1 NA 600 NA NA #&gt; 5 2013 1 2 NA 1540 NA NA #&gt; 6 2013 1 2 NA 1620 NA NA #&gt; # ... with 8,249 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Since arr_time is also missing, these are cancelled flights. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE &amp; NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!) NA ^ 0 == 1 since for all numeric values \\(x ^ 0 = 1\\). NA ^ 0 #&gt; [1] 1 NA | TRUE is TRUE because the it doesn’t matter whether the missing value is TRUE or FALSE, x \\lor T = T for all values of x. NA | TRUE #&gt; [1] TRUE Likewise, anything and FALSE is always FALSE. NA &amp; FALSE #&gt; [1] FALSE Because the value of the missing element matters in NA | FALSE and NA &amp; TRUE, these are missing: NA | FALSE #&gt; [1] NA NA &amp; TRUE #&gt; [1] NA wut? Since x * 0 = 0 for all \\(x\\) we might expect NA * 0 = 0, but that’s not the case. NA * 0 #&gt; [1] NA 3.4 Arrange missing values always at the end. 3.4.1 Exercises How could you use arrange() to sort all missing values to the start? (Hint: use is.na()). This sorts by increasing dep_time, but with all missing values put first. arrange(flights, desc(is.na(dep_time)), dep_time) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA #&gt; 2 2013 1 1 NA 1935 NA NA #&gt; 3 2013 1 1 NA 1500 NA NA #&gt; 4 2013 1 1 NA 600 NA NA #&gt; 5 2013 1 2 NA 1540 NA NA #&gt; 6 2013 1 2 NA 1620 NA NA #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Sort flights to find the most delayed flights. Find the flights that left earliest. The most delayed flights are found by sorting by dep_delay in descending order. arrange(flights, desc(dep_delay)) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 #&gt; 2 2013 6 15 1432 1935 1137 1607 #&gt; 3 2013 1 10 1121 1635 1126 1239 #&gt; 4 2013 9 20 1139 1845 1014 1457 #&gt; 5 2013 7 22 845 1600 1005 1044 #&gt; 6 2013 4 10 1100 1900 960 1342 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; If we sort dep_delay in ascending order, we get those that left earliest. There was a flight that left 43 minutes early. arrange(flights, dep_delay) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 12 7 2040 2123 -43 40 #&gt; 2 2013 2 3 2022 2055 -33 2240 #&gt; 3 2013 11 10 1408 1440 -32 1549 #&gt; 4 2013 1 11 1900 1930 -30 2233 #&gt; 5 2013 1 29 1703 1730 -27 1947 #&gt; 6 2013 8 9 729 755 -26 1002 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Sort flights to find the fastest flights. I assume that by by “fastest flights” it means the flights with the minimum air time. So I sort by air_time. The fastest flights. The fastest flights area couple of flights between EWR and BDL with an air time of 20 minutes. arrange(flights, air_time) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 16 1355 1315 40 1442 #&gt; 2 2013 4 13 537 527 10 622 #&gt; 3 2013 12 6 922 851 31 1021 #&gt; 4 2013 2 3 2153 2129 24 2247 #&gt; 5 2013 2 5 1303 1315 -12 1342 #&gt; 6 2013 2 12 2123 2130 -7 2211 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Which flights travelled the longest? Which travelled the shortest? I’ll assume hat travelled the longest or shortest refers to distance, rather than air-time. The longest flights are the Hawaii Air (HA 51) between JFK and HNL (Honolulu) at 4,983 miles. arrange(flights, desc(distance)) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 857 900 -3 1516 #&gt; 2 2013 1 2 909 900 9 1525 #&gt; 3 2013 1 3 914 900 14 1504 #&gt; 4 2013 1 4 900 900 0 1516 #&gt; 5 2013 1 5 858 900 -2 1519 #&gt; 6 2013 1 6 1019 900 79 1558 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Apart from an EWR to LGA flight that was cancelled, the shortest flights are the Envoy Air Flights between EWR and PHL at 80 miles. arrange(flights, distance) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 7 27 NA 106 NA NA #&gt; 2 2013 1 3 2127 2129 -2 2222 #&gt; 3 2013 1 4 1240 1200 40 1333 #&gt; 4 2013 1 4 1829 1615 134 1937 #&gt; 5 2013 1 4 2128 2129 -1 2218 #&gt; 6 2013 1 5 1155 1200 -5 1241 #&gt; # ... with 3.368e+05 more rows, and 12 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. A few ways include: select(flights, dep_time, dep_delay, arr_time, arr_delay) #&gt; # A tibble: 336,776 × 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # ... with 3.368e+05 more rows select(flights, starts_with(&quot;dep_&quot;), starts_with(&quot;arr_&quot;)) #&gt; # A tibble: 336,776 × 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # ... with 3.368e+05 more rows select(flights, matches(&quot;^(dep|arr)_(time|delay)$&quot;)) #&gt; # A tibble: 336,776 × 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # ... with 3.368e+05 more rows using ends_with() doesn’t work well since it would bget sched_arr_time and sched_dep_time. What happens if you include the name of a variable multiple times in a select() call? It ignores the duplicates, and that variable is only included once. No error, warning, or message is emited. select(flights, year, month, day, year, year) #&gt; # A tibble: 336,776 × 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # ... with 3.368e+05 more rows What does the one_of() function do? Why might it be helpful in conjunction with this vector? The one_of vector allows you to select variables with a character vector rather than as unquoted variable names. It’s useful because then you can easily pass vectors to select(). vars &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;dep_delay&quot;, &quot;arr_delay&quot;) select(flights, one_of(vars)) #&gt; # A tibble: 336,776 × 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # ... with 3.368e+05 more rows Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? select(flights, contains(&quot;TIME&quot;)) #&gt; # A tibble: 336,776 × 6 #&gt; dep_time sched_dep_time arr_time sched_arr_time air_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 515 830 819 227 #&gt; 2 533 529 850 830 227 #&gt; 3 542 540 923 850 160 #&gt; 4 544 545 1004 1022 183 #&gt; 5 554 600 812 837 116 #&gt; 6 554 558 740 728 150 #&gt; # ... with 3.368e+05 more rows, and 1 more variables: time_hour &lt;dttm&gt; The default behavior for contains is to ignore case. Yes, it surprises me. Upon reflection, I realized that this is likely the default behavior because dplyr is designed to deal with a variety of data backends, and some database engines don’t differentiate case. To change the behavior add the argument ignore.case = FALSE. Now no variables are selected. select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) #&gt; # A tibble: 336,776 × 0 3.5 Mutate 3.5.1 Exercises Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. To get the departure times in the number of minutes, (integer) divide dep_time by 100 to get the hours since midnight and muliply by 60 and add the remainder of dep_time divided by 100. mutate(flights, dep_time_mins = dep_time %/% 100 * 60 + dep_time %% 100, sched_dep_time_mins = sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %&gt;% select(dep_time, dep_time_mins, sched_dep_time, sched_dep_time_mins) #&gt; # A tibble: 336,776 × 4 #&gt; dep_time dep_time_mins sched_dep_time sched_dep_time_mins #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 317 515 315 #&gt; 2 533 333 529 329 #&gt; 3 542 342 540 340 #&gt; 4 544 344 545 345 #&gt; 5 554 354 600 360 #&gt; 6 554 354 558 358 #&gt; # ... with 3.368e+05 more rows This would be more cleanly done by first definining a funciton and reusing that: time2mins &lt;- function(x) { x %/% 100 * 60 + x %% 100 } mutate(flights, dep_time_mins = time2mins(dep_time), sched_dep_time_mins = time2mins(sched_dep_time)) %&gt;% select(dep_time, dep_time_mins, sched_dep_time, sched_dep_time_mins) #&gt; # A tibble: 336,776 × 4 #&gt; dep_time dep_time_mins sched_dep_time sched_dep_time_mins #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 317 515 315 #&gt; 2 533 333 529 329 #&gt; 3 542 342 540 340 #&gt; 4 544 344 545 345 #&gt; 5 554 354 600 360 #&gt; 6 554 354 558 358 #&gt; # ... with 3.368e+05 more rows Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Since arr_time and dep_time may be in different time zones, the air_time doesn’t equal the difference. We would need to account for time-zones in these calculations. mutate(flights, air_time2 = arr_time - dep_time, air_time_diff = air_time2 - air_time) %&gt;% filter(air_time_diff != 0) %&gt;% select(air_time, air_time2, dep_time, arr_time, dest) #&gt; # A tibble: 326,128 × 5 #&gt; air_time air_time2 dep_time arr_time dest #&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 227 313 517 830 IAH #&gt; 2 227 317 533 850 IAH #&gt; 3 160 381 542 923 MIA #&gt; 4 183 460 544 1004 BQN #&gt; 5 116 258 554 812 ATL #&gt; 6 150 186 554 740 ORD #&gt; # ... with 3.261e+05 more rows Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? I’d expect dep_time, sched_dep_time, and dep_delay to be related so that dep_time - sched_dep_time = dep_delay. mutate(flights, dep_delay2 = dep_time - sched_dep_time) %&gt;% filter(dep_delay2 != dep_delay) %&gt;% select(dep_time, sched_dep_time, dep_delay, dep_delay2) #&gt; # A tibble: 99,777 × 4 #&gt; dep_time sched_dep_time dep_delay dep_delay2 #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 554 600 -6 -46 #&gt; 2 555 600 -5 -45 #&gt; 3 557 600 -3 -43 #&gt; 4 557 600 -3 -43 #&gt; 5 558 600 -2 -42 #&gt; 6 558 600 -2 -42 #&gt; # ... with 9.977e+04 more rows Oops, I forgot to convert to minutes. I’ll reuse the time2mins function I wrote earlier. mutate(flights, dep_delay2 = time2mins(dep_time) - time2mins(sched_dep_time)) %&gt;% filter(dep_delay2 != dep_delay) %&gt;% select(dep_time, sched_dep_time, dep_delay, dep_delay2) #&gt; # A tibble: 1,207 × 4 #&gt; dep_time sched_dep_time dep_delay dep_delay2 #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 848 1835 853 -587 #&gt; 2 42 2359 43 -1397 #&gt; 3 126 2250 156 -1284 #&gt; 4 32 2359 33 -1407 #&gt; 5 50 2145 185 -1255 #&gt; 6 235 2359 156 -1284 #&gt; # ... with 1,201 more rows Well, that solved most of the problems, but these two numbers don’t match because we aren’t accounting for flights where the departure time is the next day from the scheduled departure time. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank(). I’d want to handle ties by taking the minimum of tied values. If three flights are have the same value and are the most delayed, we would say they are tied for first, not tied for third or second. mutate(flights, dep_delay_rank = min_rank(-dep_delay)) %&gt;% arrange(dep_delay_rank) %&gt;% filter(dep_delay_rank &lt;= 10) #&gt; # A tibble: 10 × 20 #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 #&gt; 2 2013 6 15 1432 1935 1137 1607 #&gt; 3 2013 1 10 1121 1635 1126 1239 #&gt; 4 2013 9 20 1139 1845 1014 1457 #&gt; 5 2013 7 22 845 1600 1005 1044 #&gt; 6 2013 4 10 1100 1900 960 1342 #&gt; # ... with 4 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, #&gt; # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, dep_delay_rank &lt;int&gt; What does 1:3 + 1:10 return? Why? It returns c(1 + 1, 2 + 2, 3 + 3, 1 + 4, 2 + 5, 3 + 6, 1 + 7, 2 + 8, 3 + 9, 1 + 10). When adding two vectors recycles the shorter vector’s values to get vectors of the same length. We get a warning vector since the shorter vector is not a multiple of the longer one (this often, but not necessarily, means we made an error somewhere). 1:3 + 1:10 #&gt; Warning in 1:3 + 1:10: longer object length is not a multiple of shorter #&gt; object length #&gt; [1] 2 4 6 5 7 9 8 10 12 11 What trigonometric functions does R provide? All the classics: cos, sin, tan, acos, asin, atan, plus a few others that are drive by numerical or computational issues. 3.6 Grouped summaries with summarise() 3.6.1 Exercises Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. A flight is always 10 minutes late. A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay? Arrival delay is more important. Arriving early is nice, but equally as good as arriving late is bad. Variation is worse than consistency; if I know the plane will always arrive 10 minutes late, then I can plan for it arriving as if the actual arrival time was 10 minutes later than the scheduled arrival time. So I’d try something that calculates the expected time of the flight, and then aggregates over any delays from that time. I would ignore any early arrival times. A better ranking would also consider cancellations, and need a way to convert them to a delay time (perhaps using the arrival time of the next flight to the same destination). Come up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()). Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay)) is slightly suboptimal. Why? Which is the most important column? If a flight doesn’t depart, then it won’t arrive. A flight can also depart and not arrive if it crashes; I’m not sure how this data would handle flights that are redirected and land at other airports for whatever reason. The more important column is arr_delay so we could just use that. filter(flights, !is.na(dep_delay), is.na(arr_delay)) %&gt;% select(dep_time, arr_time, sched_arr_time, dep_delay, arr_delay) #&gt; # A tibble: 1,175 × 5 #&gt; dep_time arr_time sched_arr_time dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1525 1934 1805 -5 NA #&gt; 2 1528 2002 1647 29 NA #&gt; 3 1740 2158 2020 -5 NA #&gt; 4 1807 2251 2103 29 NA #&gt; 5 1939 29 2151 59 NA #&gt; 6 1952 2358 2207 22 NA #&gt; # ... with 1,169 more rows Okay, I’m not sure what’s going on in this data. dep_time can be non-missing and arr_delay missing but arr_time not missing. They may be combining different flights? Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? cancelled_delayed &lt;- flights %&gt;% mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %&gt;% group_by(year, month, day) %&gt;% summarise(prop_cancelled = mean(cancelled), avg_dep_delay = mean(dep_delay, na.rm = TRUE)) ggplot(cancelled_delayed, aes(x = avg_dep_delay, prop_cancelled)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %&gt;% group_by(carrier, dest) %&gt;% summarise(n())) flights %&gt;% group_by(carrier) %&gt;% summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% arrange(desc(arr_delay)) #&gt; # A tibble: 16 × 2 #&gt; carrier arr_delay #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 F9 21.9 #&gt; 2 FL 20.1 #&gt; 3 EV 15.8 #&gt; 4 YV 15.6 #&gt; 5 OO 11.9 #&gt; 6 MQ 10.8 #&gt; # ... with 10 more rows filter(airlines, carrier == &quot;F9&quot;) #&gt; # A tibble: 1 × 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 F9 Frontier Airlines Inc. Frontier Airlines (FL) has the worst delays. You can get part of the way to disentangling the effects of airports vs. carriers by comparing each flight’s delay to the average delay of destination airport. However, you’d really want to compare it to the average delay of the desination airport, after removing other flights from the same airline. 538 has done something like this: http://fivethirtyeight.com/features/the-best-and-worst-airlines-airports-and-flights-summer-2015-update/. For each plane, count the number of flights before the first delay of greater than 1 hour. I think this requires grouped mutate (but I may be wrong): flights %&gt;% arrange(tailnum, year, month, day) %&gt;% group_by(tailnum) %&gt;% mutate(delay_gt1hr = dep_delay &gt; 60) %&gt;% mutate(before_delay = cumsum(delay_gt1hr)) %&gt;% filter(before_delay &lt; 1) %&gt;% count(sort = TRUE) #&gt; # A tibble: 3,755 × 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 N954UW 206 #&gt; 2 N952UW 163 #&gt; 3 N957UW 142 #&gt; 4 N5FAAA 117 #&gt; 5 N38727 99 #&gt; 6 N3742C 98 #&gt; # ... with 3,749 more rows What does the sort argument to count() do. When might you use it? The sort argument to count sorts the results in order of n. You could use this anytime you would do count followed by arrange. 3.7 Grouped mutates and filters 3.7.1 Exercises Refer back to the table of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping. They operate within each group rather than over the entire data frame. E.g. mean will calculate the mean within each group. Which plane (tailnum) has the worst on-time record? flights %&gt;% group_by(tailnum) %&gt;% summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(rank(desc(arr_delay)) &lt;= 1) #&gt; # A tibble: 1 × 2 #&gt; tailnum arr_delay #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 N844MH 320 What time of day should you fly if you want to avoid delays as much as possible? Let’s group by hour. The earlier the better to fly. This is intuitive as delays early in the morning are likely to propogate throughout the day. flights %&gt;% group_by(hour) %&gt;% summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% arrange(arr_delay) #&gt; # A tibble: 20 × 2 #&gt; hour arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 7 -5.304 #&gt; 2 5 -4.797 #&gt; 3 6 -3.384 #&gt; 4 9 -1.451 #&gt; 5 8 -1.113 #&gt; 6 10 0.954 #&gt; # ... with 14 more rows For each destination, compute the total minutes of delay. For each, flight, compute the proportion of the total delay for its destination. flights %&gt;% filter(!is.na(arr_delay), arr_delay &gt; 0) %&gt;% group_by(dest) %&gt;% mutate(total_delay = sum(arr_delay), prop_delay = arr_delay / sum(arr_delay)) #&gt; Source: local data frame [133,004 x 21] #&gt; Groups: dest [103] #&gt; #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 554 558 -4 740 #&gt; 5 2013 1 1 555 600 -5 913 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # ... with 1.33e+05 more rows, and 14 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, #&gt; # total_delay &lt;dbl&gt;, prop_delay &lt;dbl&gt; Alternatively, consider the delay as relative to the minimum delay for any flight to that destination. Now all non-cancelled flights have a proportion. flights %&gt;% filter(!is.na(arr_delay), arr_delay &gt; 0) %&gt;% group_by(dest) %&gt;% mutate(total_delay = sum(arr_delay - min(arr_delay)), prop_delay = arr_delay / sum(arr_delay)) #&gt; Source: local data frame [133,004 x 21] #&gt; Groups: dest [103] #&gt; #&gt; year month day dep_time sched_dep_time dep_delay arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 #&gt; 2 2013 1 1 533 529 4 850 #&gt; 3 2013 1 1 542 540 2 923 #&gt; 4 2013 1 1 554 558 -4 740 #&gt; 5 2013 1 1 555 600 -5 913 #&gt; 6 2013 1 1 558 600 -2 753 #&gt; # ... with 1.33e+05 more rows, and 14 more variables: #&gt; # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, #&gt; # total_delay &lt;dbl&gt;, prop_delay &lt;dbl&gt; Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() explore how the delay of a flight is related to the delay of the immediately preceding flight. We want to group by day to avoid taking the lag from the previous day. Also, I want to use departure delay, since this mechanism is relevant for departures. Also, I remove missing values both before and after calculating the lag delay. However, it would be interesting to ask the probability or averge delay after a cancellation. flights %&gt;% group_by(year, month, day) %&gt;% filter(!is.na(dep_delay)) %&gt;% mutate(lag_delay = lag(dep_delay)) %&gt;% filter(!is.na(lag_delay)) %&gt;% ggplot(aes(x = dep_delay, y = lag_delay)) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;gam&#39; Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? The shorter BOS and PHL flights that are 20 minutes for 30+ minutes flights seem plausible - though maybe entries of +/- a few minutes can easily create large changes. I assume that departure time has a standardized definition, but I’m not sure; if there is some discretion, that could create errors that are small in absolute time, but large in relative time for small flights. The ATL, GSP, an BNA flights looks a little suspicious as it’s almost half the time for longer flights. flights %&gt;% filter(!is.na(air_time)) %&gt;% group_by(dest) %&gt;% mutate(med_time = median(air_time), fast = (air_time - med_time) / med_time) %&gt;% arrange(fast) %&gt;% select(air_time, med_time, fast, dep_time, sched_dep_time, arr_time, sched_arr_time) %&gt;% head(15) #&gt; Adding missing grouping variables: `dest` #&gt; Source: local data frame [15 x 8] #&gt; Groups: dest [9] #&gt; #&gt; dest air_time med_time fast dep_time sched_dep_time arr_time #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 BOS 21 38 -0.447 1450 1500 1547 #&gt; 2 ATL 65 112 -0.420 1709 1700 1923 #&gt; 3 GSP 55 92 -0.402 2040 2025 2225 #&gt; 4 BOS 23 38 -0.395 1954 2000 2131 #&gt; 5 BNA 70 113 -0.381 1914 1910 2045 #&gt; 6 MSP 93 149 -0.376 1558 1513 1745 #&gt; # ... with 9 more rows, and 1 more variables: sched_arr_time &lt;int&gt; I could also try a z-score. Though the sd and mean will be affected by large delays. flights %&gt;% filter(!is.na(air_time)) %&gt;% group_by(dest) %&gt;% mutate(air_time_mean = mean(air_time), air_time_sd = sd(air_time), z_score = (air_time - air_time_mean) / air_time_sd) %&gt;% arrange(z_score) %&gt;% select(z_score, air_time_mean, air_time_sd, air_time, dep_time, sched_dep_time, arr_time, sched_arr_time) #&gt; Adding missing grouping variables: `dest` #&gt; Source: local data frame [327,346 x 9] #&gt; Groups: dest [104] #&gt; #&gt; dest z_score air_time_mean air_time_sd air_time dep_time sched_dep_time #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 MSP -4.90 150.6 11.75 93 1558 1513 #&gt; 2 ATL -4.88 112.9 9.81 65 1709 1700 #&gt; 3 GSP -4.72 93.4 8.13 55 2040 2025 #&gt; 4 BNA -4.05 114.4 10.96 70 1914 1910 #&gt; 5 CVG -3.98 96.0 8.52 62 1359 1343 #&gt; 6 BOS -3.63 39.0 4.95 21 1450 1500 #&gt; # ... with 3.273e+05 more rows, and 2 more variables: arr_time &lt;int&gt;, #&gt; # sched_arr_time &lt;int&gt; flights %&gt;% filter(!is.na(air_time)) %&gt;% group_by(dest) %&gt;% mutate(air_time_diff = air_time - min(air_time)) %&gt;% arrange(desc(air_time_diff)) %&gt;% select(dest, year, month, day, carrier, flight, air_time_diff, air_time, dep_time, arr_time) %&gt;% head() #&gt; Source: local data frame [6 x 10] #&gt; Groups: dest [5] #&gt; #&gt; dest year month day carrier flight air_time_diff air_time dep_time #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 SFO 2013 7 28 DL 841 195 490 1727 #&gt; 2 LAX 2013 11 22 DL 426 165 440 1812 #&gt; 3 EGE 2013 1 28 AA 575 163 382 1806 #&gt; 4 DEN 2013 9 10 UA 745 149 331 1513 #&gt; 5 LAX 2013 7 10 DL 17 147 422 1814 #&gt; 6 LAS 2013 11 22 UA 587 143 399 2142 #&gt; # ... with 1 more variables: arr_time &lt;int&gt; Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. The carrier tha flies to the most locations is ExpressJet Airlines (EV). ExpressJet is a regional airline and partner for major airlines, so its one of those that flies small planes to close airports flights %&gt;% group_by(dest, carrier) %&gt;% count(carrier) %&gt;% group_by(carrier) %&gt;% count(sort = TRUE) #&gt; # A tibble: 16 × 2 #&gt; carrier nn #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 EV 61 #&gt; 2 9E 49 #&gt; 3 UA 47 #&gt; 4 B6 42 #&gt; 5 DL 40 #&gt; 6 MQ 20 #&gt; # ... with 10 more rows filter(airlines, carrier == &quot;EV&quot;) #&gt; # A tibble: 1 × 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 EV ExpressJet Airlines Inc. "],
["exploratory-data-analysis.html", "4 Exploratory Data Analysis 4.1 Introduction 4.2 Missing Values 4.3 Covariation", " 4 Exploratory Data Analysis 4.1 Introduction library(&quot;tidyverse&quot;) 4.1.1 Questions 4.1.2 Variation 4.1.2.1 Exercises 1. Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. In order to make it eaiser to plot them, I’ll reshape the dataset so that I can use the variables as facets. diamonds %&gt;% mutate(id = row_number()) %&gt;% select(x, y, z, id) %&gt;% gather(variable, value, -id) %&gt;% ggplot(aes(x = value)) + geom_density() + geom_rug() + facet_grid(variable ~ .) There several noticeable features of thedistributions They are right skewed, with most diamonds small, but a few very large ones. There is an outlier in y, and z (see the rug) All three distributions have a bimodality (perhaps due to some sort of threshhold) According to the documentation for diamonds: x is length, y is width, and z is depth. I don’t know if I would have figured that out before; maybe if there was data on the type of cuts. 2. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.) The price data is spikey, but I can’t tell what that corresponds to, as the following plots don’t show much difference in the distributions in the last one and last two digits. There are no diamonds with a price of 1500 There’s a bulge in the distribution around 7500. ggplot(filter(diamonds, price &lt; 2500), aes(x = price)) + geom_histogram(binwidth = 10, center = 0) ggplot(filter(diamonds), aes(x = price)) + geom_histogram(binwidth = 100, center = 0) Distribution of last digit diamonds %&gt;% mutate(ending = price %% 10) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1, center = 0) + geom_bar() diamonds %&gt;% mutate(ending = price %% 100) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1) + geom_bar() diamonds %&gt;% mutate(ending = price %% 1000) %&gt;% filter(ending &gt;= 500, ending &lt;= 800) %&gt;% ggplot(aes(x = ending)) + geom_histogram(binwidth = 1) + geom_bar() 3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? There are more than 70 times as many 1 carat diamonds as 0.99 carat diamond. diamonds %&gt;% filter(carat &gt;= 0.99, carat &lt;= 1) %&gt;% count(carat) #&gt; # A tibble: 2 × 2 #&gt; carat n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.99 23 #&gt; 2 1.00 1558 I don’t know exactly the process behind how carats are measured, but some way or another some diamonds carat values are being “rounded up”, because presumably there is a premium for a 1 carat diamond vs. a 0.99 carat diamond beyond the expected increase in price due to a 0.01 carat increase. To check this intuition, we’d want to look at the number of diamonds in each carat range to seem if there is an abnormally low number at 0.99 carats, and an abnormally high number at 1 carat. diamonds %&gt;% filter(carat &gt;= 0.9, carat &lt;= 1.1) %&gt;% count(carat) %&gt;% print(n = 30) #&gt; # A tibble: 21 × 2 #&gt; carat n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.90 1485 #&gt; 2 0.91 570 #&gt; 3 0.92 226 #&gt; 4 0.93 142 #&gt; 5 0.94 59 #&gt; 6 0.95 65 #&gt; 7 0.96 103 #&gt; 8 0.97 59 #&gt; 9 0.98 31 #&gt; 10 0.99 23 #&gt; 11 1.00 1558 #&gt; 12 1.01 2242 #&gt; 13 1.02 883 #&gt; 14 1.03 523 #&gt; 15 1.04 475 #&gt; 16 1.05 361 #&gt; 17 1.06 373 #&gt; 18 1.07 342 #&gt; 19 1.08 246 #&gt; 20 1.09 287 #&gt; 21 1.10 278 Q Can you think of other examples of similar phenoma where you might expect to see similar discontinuities in areas related to your research. Compare and contrast coord_cartesian() vs xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows? coord_cartesian simply zooms in on the area specified by the limits. The calculation of the histogram is unaffected. ggplot(diamonds) + geom_histogram(mapping = aes(x = price)) + coord_cartesian(xlim = c(100, 5000), ylim = c(0, 3000)) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. However, the xlim and ylim functions first drop any values outside the limits (the ylim doesn’t matter in this case), then calculates the histogram, and draws the graph with the given limits. ggplot(diamonds) + geom_histogram(mapping = aes(x = price)) + xlim(100, 5000) + ylim(0, 3000) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 14714 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 5 rows containing missing values (geom_bar). 4.2 Missing Values 4.2.1 Exercises What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference? Missing values are removed when the number of observations in each bin are calculated. See the warning message: Removed 9 rows containing non-finite values (stat_bin) diamonds2 &lt;- diamonds %&gt;% mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y)) ggplot(diamonds2, aes(x = y)) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 9 rows containing non-finite values (stat_bin). In geom_bar, NA is treated as another category. This is because the x aesthetic in geom_bar should be a discrete (categorical) variable, and missing values are just another category. diamonds %&gt;% mutate(cut = if_else(runif(n()) &lt; 0.1, NA_character_, as.character(cut))) %&gt;% ggplot() + geom_bar(mapping = aes(x = cut)) In a histogram, the x aesthetic variable needs to be numeric, and stat_bin groups the observations by ranges into bins. Since the numeric value of the NA observations is unknown, they cannot be placed in a particular bin, and are dropped. What does na.rm = TRUE do in mean() and sum()? This option removes NA values from the vector prior to calculating the mean and sum. mean(c(0, 1, 2, NA), na.rm = TRUE) #&gt; [1] 1 sum(c(0, 1, 2, NA), na.rm = TRUE) #&gt; [1] 3 4.3 Covariation 4.3.1 A categorical and continuous variable 4.3.2 Two categorical variables 4.3.3 Two continuous variables "]
]
